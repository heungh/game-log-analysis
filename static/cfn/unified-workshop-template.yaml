AWSTemplateFormatVersion: '2010-09-09'
Description: 'Unified CloudFormation template for AWS Workshop with VS Code Server and Data Pipeline (Version 1.0.0)'

Parameters:
  # Instance Configuration
  InstanceType:
    Type: String
    Default: t3.medium
    AllowedValues: [t3.micro, t3.small, t3.medium, t3.large, m5.large, m5.xlarge, t4g.medium, t4g.large]
    Description: EC2 instance type (t3.medium recommended for stability)
    
  InstanceVolumeSize:
    Type: Number
    Description: EC2 instance volume size in GB
    Default: 40
    MinValue: 20
    MaxValue: 100

  # VS Code Configuration  
  VSCodeUser:
    Type: String
    Description: Username for VS Code server
    Default: participant
    
  HomeFolder:
    Type: String
    Description: Folder to open in VS Code server
    Default: /workshop

  # Security Configuration
  KeyName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Name of an existing EC2 KeyPair to enable SSH access

  AllowedCIDR:
    Type: String
    Default: 0.0.0.0/0
    Description: CIDR block allowed to access services (default allows all)

  # Workshop Assets
  AssetZipS3Path:
    Description: S3 path holding the asset zip file to be copied into the home folder. To not include any assets, leave blank
    Type: String
    Default: ''

  RepoUrl:
    Description: Remote repo URL to clone. To not clone a remote repo, leave blank
    Type: String
    Default: ''

Conditions:
  # Architecture detection based on instance type
  IsGraviton: !Or
    - !Equals [ !Select [ 0, !Split ['.', !Ref InstanceType ]], 't4g']
    - !Equals [ !Select [ 0, !Split ['.', !Ref InstanceType ]], 'c6g']
    - !Equals [ !Select [ 0, !Split ['.', !Ref InstanceType ]], 'c7g']
    - !Equals [ !Select [ 0, !Split ['.', !Ref InstanceType ]], 'm6g']
    - !Equals [ !Select [ 0, !Split ['.', !Ref InstanceType ]], 'm7g']

Mappings:
  # AMI mappings using SSM parameters for latest images
  ArmImage:
    AmazonLinux-2023:
      ImageId: '{{resolve:ssm:/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-arm64}}'
  
  AmdImage:
    AmazonLinux-2023:
      ImageId: '{{resolve:ssm:/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64}}'
  
  # CloudFront Prefix Lists by region
  AWSRegionsPrefixListID:
    ap-northeast-1:
      PrefixList: pl-58a04531
    ap-northeast-2:
      PrefixList: pl-22a6434b
    ap-south-1:
      PrefixList: pl-9aa247f3
    ap-southeast-1:
      PrefixList: pl-31a34658
    ap-southeast-2:
      PrefixList: pl-b8a742d1
    ca-central-1:
      PrefixList: pl-38a64351
    eu-central-1:
      PrefixList: pl-a3a144ca
    eu-north-1:
      PrefixList: pl-fab65393
    eu-west-1:
      PrefixList: pl-4fa04526
    eu-west-2:
      PrefixList: pl-93a247fa
    eu-west-3:
      PrefixList: pl-75b1541c
    sa-east-1:
      PrefixList: pl-5da64334
    us-east-1:
      PrefixList: pl-3b927c52
    us-east-2:
      PrefixList: pl-b6a144df
    us-west-1:
      PrefixList: pl-4ea04527
    us-west-2:
      PrefixList: pl-82a045eb

Resources:
  # VPC and Networking Infrastructure
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-VPC'

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-IGW'

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 0, !GetAZs '' ]
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PublicSubnet1'

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 1, !GetAZs '' ]
      CidrBlock: 10.0.2.0/24
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PublicSubnet2'

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 0, !GetAZs '' ]
      CidrBlock: 10.0.3.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PrivateSubnet1'

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 1, !GetAZs '' ]
      CidrBlock: 10.0.4.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PrivateSubnet2'

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-PublicRouteTable'

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: AttachGateway
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref PublicRouteTable
  # Security Groups
  WorkshopSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Workshop instance (VS Code + Data Pipeline)
      VpcId: !Ref VPC
      SecurityGroupIngress:
        # SSH access
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref AllowedCIDR
          Description: SSH access
        # VS Code Server access
        - IpProtocol: tcp
          FromPort: 8080
          ToPort: 8080
          CidrIp: !Ref AllowedCIDR
          Description: VS Code Server access
        # Streamlit access
        - IpProtocol: tcp
          FromPort: 8501
          ToPort: 8501
          CidrIp: !Ref AllowedCIDR
          Description: Streamlit access
        # Data Pipeline internal ports
        - IpProtocol: tcp
          FromPort: 24224
          ToPort: 24224
          CidrIp: 10.0.0.0/16
          Description: Fluent Bit forward input
        - IpProtocol: tcp
          FromPort: 8686
          ToPort: 8686
          CidrIp: 10.0.0.0/16
          Description: Vector API
        - IpProtocol: tcp
          FromPort: 2020
          ToPort: 2020
          CidrIp: 10.0.0.0/16
          Description: Fluent Bit HTTP
        # MySQL access
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          CidrIp: 10.0.0.0/16
          Description: MySQL access
        # CloudFront access
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourcePrefixListId: !FindInMap [AWSRegionsPrefixListID, !Ref 'AWS::Region', PrefixList]
          Description: CloudFront access
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: All outbound traffic
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-SecurityGroup'

  # Secrets Manager for VS Code and Database
  VSCodeSecret:
    Type: AWS::SecretsManager::Secret
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      Name: !Sub '${AWS::StackName}-vscode-secret'
      Description: VS Code server user details
      GenerateSecretString:
        PasswordLength: 16
        SecretStringTemplate: !Sub '{"username":"${VSCodeUser}"}'
        GenerateStringKey: 'password'
        ExcludePunctuation: true

  AuroraSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub '${AWS::StackName}-aurora-secret'
      Description: Secrets for Aurora MySQL Cluster
      SecretString: !Sub 
        - '{"username":"dbadmin","password":"12345678","engine":"mysql","host":"gamedb1cluster.cluster-${Region}.rds.amazonaws.com","port":3306,"dbClusterIdentifier":"gamedb1cluster","dbname":"game"}'
        - Region: !Ref 'AWS::Region'

  # Lambda function for secret processing
  SecretPlaintextLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub lambda.${AWS::URLSuffix}
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SecretsManagerAccess
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                Resource: 
                  - !Ref VSCodeSecret
                  - !Ref AuroraSecret

  SecretPlaintextLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Return the value of the secret
      Handler: index.lambda_handler
      Runtime: python3.13
      MemorySize: 128
      Timeout: 10
      Architectures:
        - arm64
      Role: !GetAtt SecretPlaintextLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import cfnresponse
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def is_valid_json(json_string):
              try:
                  json.loads(json_string)
                  return True
              except json.JSONDecodeError:
                  return False

          def lambda_handler(event, context):
              try:
                  if event['RequestType'] == 'Delete':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData={}, reason='No action to take')
                  else:
                      resource_properties = event['ResourceProperties']
                      secret_name = resource_properties['SecretArn']
                      secrets_mgr = boto3.client('secretsmanager')

                      secret = secrets_mgr.get_secret_value(SecretId = secret_name)
                      secret_value = secret['SecretString']

                      responseData = {}
                      if is_valid_json(secret_value):
                          responseData = secret_value
                      else:
                          responseData = {'secret': secret_value}
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData=json.loads(responseData), reason='OK', noEcho=True)
              except Exception as e:
                  logger.error(e)
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData={}, reason=str(e))

  SecretPlaintext:
    Type: Custom::SecretPlaintextLambda
    Properties:
      ServiceToken: !GetAtt SecretPlaintextLambda.Arn
      ServiceTimeout: 15
      SecretArn: !Ref VSCodeSecret

  # S3 Bucket for data storage
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'workshop-data-${AWS::AccountId}-${AWS::Region}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-DataBucket'

  # S3 VPC Endpoint
  S3Endpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal: '*'
            Action:
              - 's3:*'
            Resource:
              - !Sub 'arn:aws:s3:::${S3Bucket}'
              - !Sub 'arn:aws:s3:::${S3Bucket}/*'
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcId: !Ref VPC
      RouteTableIds:
        - !Ref PublicRouteTable

  # Aurora MySQL Cluster
  AuroraDBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet group for Aurora DB
      SubnetIds: 
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-DBSubnetGroup'

  AuroraDBCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      DatabaseName: game
      DBClusterIdentifier: !Sub '${AWS::StackName}-gamedb-cluster'
      MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref AuroraSecret, ':SecretString:username}}' ]]
      MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref AuroraSecret, ':SecretString:password}}' ]]
      DBSubnetGroupName: !Ref AuroraDBSubnetGroup
      VpcSecurityGroupIds: 
        - !Ref WorkshopSecurityGroup
      BackupRetentionPeriod: 1
      DeletionProtection: false
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-AuroraCluster'

  AuroraDBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      Engine: aurora-mysql
      DBClusterIdentifier: !Ref AuroraDBCluster
      DBInstanceClass: db.r5.large
      PubliclyAccessible: false
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-AuroraInstance'

  # CloudWatch Log Groups
  WorkshopLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/ec2/${AWS::StackName}'
      RetentionInDays: 7

  DataPipelineLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/ec2/${AWS::StackName}/data-pipeline'
      RetentionInDays: 7

  FluentBitLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/ec2/${AWS::StackName}/fluent-bit'
      RetentionInDays: 7

  # IAM Role for Workshop Instance
  WorkshopInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        - arn:aws:iam::aws:policy/PowerUserAccess
      Policies:
        - PolicyName: WorkshopInstancePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              # Kinesis access
              - Effect: Allow
                Action:
                  - kinesis:PutRecord
                  - kinesis:PutRecords
                  - kinesis:DescribeStream
                  - kinesis:ListStreams
                Resource: '*'
              # S3 access
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                Resource: 
                  - !Sub 'arn:aws:s3:::${S3Bucket}'
                  - !Sub 'arn:aws:s3:::${S3Bucket}/*'
              # CloudWatch Logs access
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:DescribeLogStreams
                  - logs:DescribeLogGroups
                Resource: '*'
              # Secrets Manager access
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                  - secretsmanager:DescribeSecret
                Resource: 
                  - !Ref VSCodeSecret
                  - !Ref AuroraSecret
              # RDS access
              - Effect: Allow
                Action:
                  - rds:DescribeDBClusters
                  - rds:DescribeDBInstances
                Resource: '*'

  WorkshopInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref WorkshopInstanceRole

  # Lambda function to update Aurora secret with endpoint
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SecretManagerAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                  - secretsmanager:PutSecretValue
                  - secretsmanager:ListSecrets
                Resource: 
                  - !Ref AuroraSecret
        - PolicyName: RDSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - rds:DescribeDBClusters
                Resource: '*'

  UpdateSecretFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json

          def handler(event, context):
            if event['RequestType'] in ['Create', 'Update']:
              secret_manager = boto3.client('secretsmanager')
              rds = boto3.client('rds')
              
              secret_id = event['ResourceProperties']['SecretId']
              cluster_id = event['ResourceProperties']['ClusterId']
              
              try:
                response = rds.describe_db_clusters(DBClusterIdentifier=cluster_id)
                endpoint = response['DBClusters'][0]['Endpoint']
                
                secret = secret_manager.get_secret_value(SecretId=secret_id)
                secret_dict = json.loads(secret['SecretString'])
                secret_dict['host'] = endpoint
                
                secret_manager.put_secret_value(
                  SecretId=secret_id,
                  SecretString=json.dumps(secret_dict)
                )
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                print(f"Error updating secret: {str(e)}")
                cfnresponse.send(event, context, cfnresponse.FAILED, {})
                return
            else:
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

  UpdateSecretCustomResource:
    Type: Custom::UpdateSecret
    DependsOn:
      - AuroraDBCluster
    Properties:
      ServiceToken: !GetAtt UpdateSecretFunction.Arn
      SecretId: !Ref AuroraSecret
      ClusterId: !Ref AuroraDBCluster

  # Main Workshop Instance (VS Code + Data Pipeline)
  WorkshopInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If
        - IsGraviton
        - !FindInMap [ArmImage, AmazonLinux-2023, ImageId]
        - !FindInMap [AmdImage, AmazonLinux-2023, ImageId]
      InstanceType: !Ref InstanceType
      KeyName: !Ref KeyName
      SubnetId: !Ref PublicSubnet1
      SecurityGroupIds:
        - !Ref WorkshopSecurityGroup
      IamInstanceProfile: !Ref WorkshopInstanceProfile
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref InstanceVolumeSize
            VolumeType: gp3
            DeleteOnTermination: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-WorkshopInstance'
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          set -e
          
          # Logging function
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a /var/log/workshop-setup.log
          }
          
          log "Starting workshop instance setup..."
          
          # Detect architecture
          ARCH=$(uname -m)
          log "Detected architecture: $ARCH"
          
          if [[ "$ARCH" == "x86_64" ]]; then
            ARCH_TYPE="amd64"
            VECTOR_ARCH="x86_64-unknown-linux-musl"
            VSCODE_ARCH="linux-amd64"
          elif [[ "$ARCH" == "aarch64" ]]; then
            ARCH_TYPE="arm64"
            VECTOR_ARCH="aarch64-unknown-linux-musl"
            VSCODE_ARCH="linux-arm64"
          else
            log "ERROR: Unsupported architecture: $ARCH"
            exit 1
          fi
          
          log "Using architecture type: $ARCH_TYPE"
          
          # Update system
          log "Updating system packages..."
          yum update -y
          
          # Install basic packages
          log "Installing basic packages..."
          yum install -y wget curl unzip tar gzip git htop tree jq net-tools
          
          # Install Docker
          log "Installing Docker..."
          yum install -y docker
          systemctl start docker
          systemctl enable docker
          usermod -a -G docker ec2-user
          
          # Install development tools and MySQL development libraries
          log "Installing development tools..."
          yum groupinstall -y "Development Tools"
          yum install -y mysql-devel
          
          # Install Node.js (for VSCode Server)
          log "Installing Node.js..."
          curl -fsSL https://rpm.nodesource.com/setup_18.x | bash -
          yum install -y nodejs
          
          # Install Python packages
          log "Installing Python packages..."
          yum install -y python3-pip python3-devel
          
          # Install Java (for Kinesis Agent)
          log "Installing Java..."
          yum install -y java-1.8.0-openjdk
          
          # Install Amazon SSM Agent
          log "Installing SSM Agent..."
          yum install -y amazon-ssm-agent
          systemctl enable amazon-ssm-agent
          systemctl start amazon-ssm-agent
          
          # Install Nginx for CloudFront integration
          log "Installing Nginx..."
          yum install -y nginx openssl argon2
          systemctl enable nginx
          
          # Install VS Code Server with architecture detection
          log "Installing VS Code Server for $ARCH_TYPE..."
          cd /opt
          wget -O code-server.tar.gz "https://github.com/coder/code-server/releases/download/v4.20.0/code-server-4.20.0-$VSCODE_ARCH.tar.gz"
          tar -xzf code-server.tar.gz
          mv "code-server-4.20.0-$VSCODE_ARCH" code-server
          ln -s /opt/code-server/bin/code-server /usr/local/bin/code-server
          rm -f code-server.tar.gz
          
          # Get VS Code password from Secrets Manager
          log "Retrieving VS Code password from Secrets Manager..."
          VSCODE_PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${VSCodeSecret} --region ${AWS::Region} --query SecretString --output text | jq -r .password)
          
          # Configure Nginx for CloudFront integration
          log "Configuring Nginx for CloudFront..."
          cat > /etc/nginx/conf.d/code-server.conf << 'EOF'
          server {
              listen 80;
              listen [::]:80;
              server_name *.cloudfront.net;
              
              # VS Code Server proxy
              location / {
                proxy_pass http://localhost:8080/;
                proxy_set_header Host $host;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection upgrade;
                proxy_set_header Accept-Encoding gzip;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
              }
              
              # Health check endpoint
              location /healthz {
                access_log off;
                return 200 "healthy\n";
                add_header Content-Type text/plain;
              }
          }
          EOF
          
          # Test Nginx configuration
          nginx -t
          systemctl start nginx
          
          # Create VS Code configuration directory
          log "Setting up VS Code configuration..."
          mkdir -p /home/${VSCodeUser}/.config/code-server
          mkdir -p /home/${VSCodeUser}/.local/share/code-server/User/
          
          # Create VS Code server configuration with hashed password
          log "Creating VS Code server configuration..."
          HASHED_PASSWORD=$(echo -n "$VSCODE_PASSWORD" | argon2 $(openssl rand -base64 12) -e)
          cat > /home/${VSCodeUser}/.config/code-server/config.yaml << EOF
          bind-addr: 127.0.0.1:8080
          auth: password
          hashed-password: "$HASHED_PASSWORD"
          cert: false
          EOF
          
          # Create VS Code user settings with security and productivity configurations
          log "Creating VS Code user settings..."
          cat > /home/${VSCodeUser}/.local/share/code-server/User/settings.json << 'EOF'
          {
            "extensions.autoUpdate": false,
            "extensions.autoCheckUpdates": false,
            "telemetry.telemetryLevel": "off",
            "security.workspace.trust.startupPrompt": "never",
            "security.workspace.trust.enabled": false,
            "security.workspace.trust.banner": "never",
            "security.workspace.trust.emptyWindow": false,
            "workbench.startupEditor": "none",
            "terminal.integrated.defaultProfile.linux": "bash",
            "terminal.integrated.profiles.linux": {
              "bash": {
                "path": "/bin/bash",
                "args": ["-l"]
              }
            },
            "python.defaultInterpreterPath": "/home/${VSCodeUser}/myenv/bin/python",
            "python.terminal.activateEnvironment": true,
            "auto-run-command.rules": [
              {
                "command": "workbench.action.terminal.new"
              }
            ]
          }
          EOF
          
          # Create VSCode service
          log "Creating VS Code service..."
          cat > /etc/systemd/system/code-server.service << 'EOF'
          [Unit]
          Description=code-server
          After=network.target
          
          [Service]
          Type=simple
          User=${VSCodeUser}
          WorkingDirectory=/home/${VSCodeUser}
          ExecStart=/usr/local/bin/code-server --config /home/${VSCodeUser}/.config/code-server/config.yaml ${HomeFolder}
          Restart=always
          RestartSec=5
          
          [Install]
          WantedBy=multi-user.target
          EOF
          
          # Set proper ownership
          chown -R ${VSCodeUser}:${VSCodeUser} /home/${VSCodeUser}/.config
          chown -R ${VSCodeUser}:${VSCodeUser} /home/${VSCodeUser}/.local
          
          systemctl daemon-reload
          systemctl enable code-server
          
          # Install Vector with architecture detection
          log "Installing Vector for $ARCH_TYPE..."
          cd /tmp
          wget "https://github.com/vectordotdev/vector/releases/download/v0.48.0/vector-0.48.0-$VECTOR_ARCH.tar.gz"
          tar -xzf "vector-0.48.0-$VECTOR_ARCH.tar.gz"
          cp "vector-$VECTOR_ARCH/bin/vector" /usr/local/bin/
          chmod +x /usr/local/bin/vector
          rm -rf vector-*
          
          # Create Vector directories
          log "Setting up Vector directories..."
          mkdir -p /etc/vector
          mkdir -p /var/lib/vector
          mkdir -p /var/log/vector
          chown ec2-user:ec2-user /var/lib/vector /var/log/vector
          
          # Create Vector configuration
          log "Creating Vector configuration..."
          cat > /etc/vector/vector.yaml << 'EOF'
          # Vector Configuration
          sources:
            file_logs:
              type: file
              include:
                - /var/log/*.log
                - /var/log/messages
              ignore_older_secs: 600
          
            host_metrics:
              type: host_metrics
              collectors:
                - cpu
                - disk
                - filesystem
                - load
                - host
                - memory
                - network
          
            fluent_forward:
              type: fluent
              address: 0.0.0.0:24224
          
          transforms:
            parse_logs:
              type: remap
              inputs:
                - file_logs
                - fluent_forward
              source: |
                .timestamp = now()
                .host = get_hostname!()
                .source_type = "vector"
          
          sinks:
            console_out:
              type: console
              inputs:
                - parse_logs
                - host_metrics
              encoding:
                codec: json
          
            file_out:
              type: file
              inputs:
                - parse_logs
              path: /var/log/vector/output-%Y-%m-%d.log
              encoding:
                codec: json
          
            cloudwatch_logs:
              type: aws_cloudwatch_logs
              inputs:
                - parse_logs
                - host_metrics
              group_name: ${DataPipelineLogGroup}
              stream_name: "{{ host }}"
              region: ${AWS::Region}
          
            s3_sink:
              type: aws_s3
              inputs:
                - parse_logs
              bucket: ${S3Bucket}
              key_prefix: "logs/year=%Y/month=%m/day=%d/"
              region: ${AWS::Region}
              encoding:
                codec: json
          EOF
          
          # Create Vector service
          log "Creating Vector service..."
          cat > /etc/systemd/system/vector.service << 'EOF'
          [Unit]
          Description=Vector
          After=network.target
          
          [Service]
          Type=simple
          User=ec2-user
          ExecStart=/usr/local/bin/vector --config /etc/vector/vector.yaml
          Restart=always
          RestartSec=5
          
          [Install]
          WantedBy=multi-user.target
          EOF
          
          # Install Fluent Bit with improved compatibility
          log "Installing Fluent Bit..."
          if [[ "$ARCH_TYPE" == "amd64" ]]; then
            # Use official installation script for x86_64
            curl https://raw.githubusercontent.com/fluent/fluent-bit/master/install.sh | sh
          else
            # For ARM64, use package manager
            log "Installing Fluent Bit via package manager for ARM64..."
            yum install -y fluent-bit || {
              log "WARNING: Fluent Bit installation failed on ARM64, skipping..."
            }
          fi
          
          # Create Fluent Bit configuration
          log "Creating Fluent Bit configuration..."
          mkdir -p /etc/fluent-bit
          cat > /etc/fluent-bit/fluent-bit.conf << 'EOF'
          [SERVICE]
              Flush         1
              Log_Level     info
              Daemon        off
              Parsers_File  parsers.conf
              HTTP_Server   On
              HTTP_Listen   0.0.0.0
              HTTP_Port     2020
          
          [INPUT]
              Name              tail
              Path              /var/log/*.log
              Path_Key          filename
              Parser            json
              Tag               host.*
              Refresh_Interval  5
              Skip_Long_Lines   On
          
          [INPUT]
              Name   cpu
              Tag    cpu.local
          
          [INPUT]
              Name   mem
              Tag    mem.local
          
          [OUTPUT]
              Name  forward
              Match *
              Host  127.0.0.1
              Port  24224
          
          [OUTPUT]
              Name  cloudwatch_logs
              Match *
              region ${AWS::Region}
              log_group_name ${FluentBitLogGroup}
              log_stream_name fluent-bit-stream
              auto_create_group true
          EOF
          
          # Install AWS Kinesis Agent
          log "Installing AWS Kinesis Agent..."
          cd /tmp
          wget https://s3.amazonaws.com/streaming-data-agent/aws-kinesis-agent-latest.amzn2.noarch.rpm
          rpm -U ./aws-kinesis-agent-latest.amzn2.noarch.rpm || {
            log "WARNING: Kinesis Agent installation failed, skipping..."
          }
          
          # Create Kinesis Agent configuration
          log "Creating Kinesis Agent configuration..."
          mkdir -p /etc/aws-kinesis
          cat > /etc/aws-kinesis/agent.json << 'EOF'
          {
            "cloudwatch.emitMetrics": true,
            "kinesis.endpoint": "",
            "firehose.endpoint": "",
            "flows": [
              {
                "filePattern": "/var/log/vector/output-*.log",
                "kinesisStream": "data-pipeline-stream",
                "partitionKeyOption": "RANDOM"
              }
            ]
          }
          EOF
          
          # Set up Python environment for ec2-user
          log "Setting up Python environment..."
          su - ec2-user << 'EOSU'
          python3 -m venv myenv
          source myenv/bin/activate
          pip install --upgrade pip
          pip install mysql-connector-python mysql mysql-connector boto3 langchain-aws streamlit pandas numpy
          
          # Create data directory
          mkdir -p ~/data
          EOSU
          
          # Set up environment variables
          log "Setting up environment variables..."
          cat >> /home/ec2-user/.bashrc << 'EOF'
          export AWS_DEFAULT_REGION=${AWS::Region}
          export S3_BUCKET=${S3Bucket}
          export DB_SECRET_ARN=${AuroraSecret}
          export PATH=$PATH:/home/ec2-user/.local/bin
          export NEXT_TELEMETRY_DISABLED=1
          EOF
          
          # Create workshop directory
          log "Setting up workshop directory..."
          mkdir -p ${HomeFolder}
          chown -R ec2-user:ec2-user ${HomeFolder}
          
          # Clone repository if specified
          if [[ -n "${RepoUrl}" ]]; then
            log "Cloning repository: ${RepoUrl}"
            sudo -u ec2-user git clone ${RepoUrl} ${HomeFolder}
          fi
          
          # Download assets if specified
          if [[ -n "${AssetZipS3Path}" ]]; then
            log "Downloading workshop assets..."
            aws s3 cp s3://${AssetZipS3Path} /tmp/assets.zip
            unzip -o /tmp/assets.zip -d ${HomeFolder}
            chown -R ec2-user:ec2-user ${HomeFolder}
            rm -f /tmp/assets.zip
          fi
          
          # Create service management scripts
          log "Creating service management scripts..."
          cat > /home/ec2-user/manage-services.sh << 'EOSCRIPT'
          #!/bin/bash
          
          # Colors
          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'
          
          show_status() {
              echo -e "${BLUE}=== Workshop Services Status ===${NC}"
              echo ""
              
              services=("code-server" "vector" "fluent-bit" "aws-kinesis-agent")
              descriptions=("VS Code Server" "Vector" "Fluent Bit" "Kinesis Agent")
              
              for i in "${!services[@]}"; do
                  service="${services[$i]}"
                  desc="${descriptions[$i]}"
                  
                  if systemctl is-active --quiet "$service" 2>/dev/null; then
                      status="${GREEN}Running${NC}"
                  else
                      status="${RED}Stopped${NC}"
                  fi
                  
                  printf "%-15s: %b\n" "$desc" "$status"
              done
              echo ""
              
              # Port status
              echo -e "${BLUE}=== Port Status ===${NC}"
              echo "VS Code (8080): $(netstat -ln 2>/dev/null | grep :8080 > /dev/null && echo -e "${GREEN}Open${NC}" || echo -e "${RED}Closed${NC}")"
              echo "Vector API (8686): $(netstat -ln 2>/dev/null | grep :8686 > /dev/null && echo -e "${GREEN}Open${NC}" || echo -e "${RED}Closed${NC}")"
              echo "Fluent Forward (24224): $(netstat -ln 2>/dev/null | grep :24224 > /dev/null && echo -e "${GREEN}Open${NC}" || echo -e "${RED}Closed${NC}")"
              echo "Fluent HTTP (2020): $(netstat -ln 2>/dev/null | grep :2020 > /dev/null && echo -e "${GREEN}Open${NC}" || echo -e "${RED}Closed${NC}")"
              echo ""
          }
          
          case "$1" in
              status)
                  show_status
                  ;;
              start)
                  echo "Starting all services..."
                  sudo systemctl start code-server vector fluent-bit aws-kinesis-agent 2>/dev/null
                  show_status
                  ;;
              stop)
                  echo "Stopping all services..."
                  sudo systemctl stop aws-kinesis-agent fluent-bit vector code-server 2>/dev/null
                  show_status
                  ;;
              restart)
                  echo "Restarting all services..."
                  sudo systemctl restart code-server vector fluent-bit aws-kinesis-agent 2>/dev/null
                  show_status
                  ;;
              *)
                  echo "Usage: $0 {start|stop|restart|status}"
                  show_status
                  ;;
          esac
          EOSCRIPT
          
          chmod +x /home/ec2-user/manage-services.sh
          chown ec2-user:ec2-user /home/ec2-user/manage-services.sh
          
          # Create enhanced password change script with Secrets Manager integration
          log "Creating enhanced password change script..."
          cat > /home/ec2-user/change-vscode-password.sh << 'EOSCRIPT'
          #!/bin/bash
          echo "=================================================="
          echo "VS Code Server Password Change Script"
          echo "=================================================="
          echo "Current password is managed by AWS Secrets Manager"
          echo ""
          echo "Please change the password for security."
          echo ""
          
          read -s -p "Enter new password: " NEW_PASSWORD
          echo ""
          read -s -p "Confirm password: " CONFIRM_PASSWORD
          echo ""
          
          if [ "$NEW_PASSWORD" != "$CONFIRM_PASSWORD" ]; then
              echo "âŒ Passwords do not match."
              exit 1
          fi
          
          if [ ${#NEW_PASSWORD} -lt 8 ]; then
              echo "âŒ Password must be at least 8 characters."
              exit 1
          fi
          
          echo "Updating password in Secrets Manager and VS Code configuration..."
          
          # Update Secrets Manager
          SECRET_ARN="${VSCodeSecret}"
          CURRENT_SECRET=$(aws secretsmanager get-secret-value --secret-id "$SECRET_ARN" --region ${AWS::Region} --query SecretString --output text)
          UPDATED_SECRET=$(echo "$CURRENT_SECRET" | jq --arg pwd "$NEW_PASSWORD" '.password = $pwd')
          
          aws secretsmanager put-secret-value --secret-id "$SECRET_ARN" --region ${AWS::Region} --secret-string "$UPDATED_SECRET"
          
          # Update VS Code configuration with hashed password
          HASHED_PASSWORD=$(echo -n "$NEW_PASSWORD" | argon2 $(openssl rand -base64 12) -e)
          cat > /home/${VSCodeUser}/.config/code-server/config.yaml << EOF
          bind-addr: 127.0.0.1:8080
          auth: password
          hashed-password: "$HASHED_PASSWORD"
          cert: false
          EOF
          
          # Restart VS Code server
          sudo systemctl restart code-server
          
          sleep 3
          if systemctl is-active --quiet code-server; then
              echo "âœ… Password updated successfully in Secrets Manager."
              echo "âœ… VS Code server configuration updated."
              echo "âœ… VS Code server restarted."
              echo ""
              echo "You can now access VS Code with the new password."
              echo "CloudFront URL: https://$(aws cloudformation describe-stacks --stack-name ${AWS::StackName} --region ${AWS::Region} --query 'Stacks[0].Outputs[?OutputKey==\`VSCodeURL\`].OutputValue' --output text 2>/dev/null || echo 'Not available')"
              echo "Direct URL: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8080"
          else
              echo "âŒ Failed to restart VS Code server."
              echo "Check service status: sudo systemctl status code-server"
              exit 1
          fi
          EOSCRIPT
          
          chmod +x /home/ec2-user/change-vscode-password.sh
          chown ec2-user:ec2-user /home/ec2-user/change-vscode-password.sh
          
          # Create data pipeline tools access setup script
          log "Creating data pipeline tools access setup script..."
          cat > /home/ec2-user/setup-data-pipeline-access.sh << 'EOSCRIPT'
          #!/bin/bash
          echo "=================================================="
          echo "Data Pipeline Tools Access Setup"
          echo "=================================================="
          echo ""
          
          # Colors
          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'
          
          log() {
            echo -e "${BLUE}[$(date '+%H:%M:%S')]${NC} $1"
          }
          
          success() {
            echo -e "${GREEN}âœ… $1${NC}"
          }
          
          warning() {
            echo -e "${YELLOW}âš ï¸  $1${NC}"
          }
          
          error() {
            echo -e "${RED}âŒ $1${NC}"
          }
          
          log "Setting up data pipeline tools access permissions..."
          
          # Setup database connection
          log "Configuring database access..."
          DB_SECRET_ARN="${AuroraSecret}"
          DB_ENDPOINT=$(aws secretsmanager get-secret-value --secret-id "$DB_SECRET_ARN" --region ${AWS::Region} --query SecretString --output text | jq -r .host)
          DB_USER=$(aws secretsmanager get-secret-value --secret-id "$DB_SECRET_ARN" --region ${AWS::Region} --query SecretString --output text | jq -r .username)
          DB_PASSWORD=$(aws secretsmanager get-secret-value --secret-id "$DB_SECRET_ARN" --region ${AWS::Region} --query SecretString --output text | jq -r .password)
          
          # Create database connection script
          cat > /home/${VSCodeUser}/connect-to-database.py << 'PYEOF'
          #!/usr/bin/env python3
          import mysql.connector
          import boto3
          import json
          import os
          
          def get_db_credentials():
              """Get database credentials from AWS Secrets Manager"""
              secret_arn = os.environ.get('DB_SECRET_ARN', '${AuroraSecret}')
              region = os.environ.get('AWS_DEFAULT_REGION', '${AWS::Region}')
              
              client = boto3.client('secretsmanager', region_name=region)
              response = client.get_secret_value(SecretId=secret_arn)
              secret = json.loads(response['SecretString'])
              
              return {
                  'host': secret['host'],
                  'user': secret['username'],
                  'password': secret['password'],
                  'database': secret['dbname']
              }
          
          def connect_to_database():
              """Connect to the Aurora MySQL database"""
              try:
                  creds = get_db_credentials()
                  connection = mysql.connector.connect(**creds)
                  print(f"âœ… Successfully connected to database: {creds['host']}")
                  return connection
              except Exception as e:
                  print(f"âŒ Failed to connect to database: {e}")
                  return None
          
          if __name__ == "__main__":
              conn = connect_to_database()
              if conn:
                  cursor = conn.cursor()
                  cursor.execute("SHOW TABLES;")
                  tables = cursor.fetchall()
                  print(f"ðŸ“Š Available tables: {tables}")
                  cursor.close()
                  conn.close()
          PYEOF
          
          chmod +x /home/${VSCodeUser}/connect-to-database.py
          chown ${VSCodeUser}:${VSCodeUser} /home/${VSCodeUser}/connect-to-database.py
          
          # Create S3 access test script
          cat > /home/${VSCodeUser}/test-s3-access.py << 'PYEOF'
          #!/usr/bin/env python3
          import boto3
          import os
          from datetime import datetime
          
          def test_s3_access():
              """Test S3 bucket access"""
              bucket_name = os.environ.get('S3_BUCKET', '${S3Bucket}')
              region = os.environ.get('AWS_DEFAULT_REGION', '${AWS::Region}')
              
              try:
                  s3 = boto3.client('s3', region_name=region)
                  
                  # Test write access
                  test_key = f"test/access-test-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt"
                  test_content = f"S3 access test at {datetime.now()}"
                  
                  s3.put_object(Bucket=bucket_name, Key=test_key, Body=test_content)
                  print(f"âœ… Successfully wrote to S3: s3://{bucket_name}/{test_key}")
                  
                  # Test read access
                  response = s3.get_object(Bucket=bucket_name, Key=test_key)
                  content = response['Body'].read().decode('utf-8')
                  print(f"âœ… Successfully read from S3: {content}")
                  
                  # Clean up
                  s3.delete_object(Bucket=bucket_name, Key=test_key)
                  print(f"âœ… Test file cleaned up")
                  
                  return True
              except Exception as e:
                  print(f"âŒ S3 access test failed: {e}")
                  return False
          
          if __name__ == "__main__":
              test_s3_access()
          PYEOF
          
          chmod +x /home/${VSCodeUser}/test-s3-access.py
          chown ${VSCodeUser}:${VSCodeUser} /home/${VSCodeUser}/test-s3-access.py
          
          # Create Vector status check script
          cat > /home/${VSCodeUser}/check-vector-status.sh << 'BASHEOF'
          #!/bin/bash
          echo "=================================================="
          echo "Vector Data Pipeline Status Check"
          echo "=================================================="
          
          # Check Vector service status
          if systemctl is-active --quiet vector; then
              echo "âœ… Vector service is running"
          else
              echo "âŒ Vector service is not running"
              echo "   Try: sudo systemctl start vector"
          fi
          
          # Check Vector API
          if curl -s http://localhost:8686/health > /dev/null 2>&1; then
              echo "âœ… Vector API is responding"
          else
              echo "âš ï¸  Vector API is not responding (this is normal if API is disabled)"
          fi
          
          # Check Vector logs
          echo ""
          echo "ðŸ“Š Recent Vector logs:"
          sudo journalctl -u vector -n 5 --no-pager
          
          # Check output files
          echo ""
          echo "ðŸ“ Vector output files:"
          ls -la /var/log/vector/ 2>/dev/null || echo "No output files found"
          BASHEOF
          
          chmod +x /home/${VSCodeUser}/check-vector-status.sh
          chown ${VSCodeUser}:${VSCodeUser} /home/${VSCodeUser}/check-vector-status.sh
          
          # Test all connections
          log "Testing data pipeline connections..."
          
          # Test database connection
          if python3 /home/${VSCodeUser}/connect-to-database.py; then
              success "Database connection test passed"
          else
              warning "Database connection test failed - database may still be initializing"
          fi
          
          # Test S3 access
          if python3 /home/${VSCodeUser}/test-s3-access.py; then
              success "S3 access test passed"
          else
              error "S3 access test failed"
          fi
          
          # Test Vector status
          if systemctl is-active --quiet vector; then
              success "Vector service is running"
          else
              warning "Vector service is not running yet"
          fi
          
          echo ""
          echo "=================================================="
          echo "Data Pipeline Tools Access Setup Complete"
          echo "=================================================="
          echo ""
          echo "Available tools and scripts:"
          echo "  â€¢ connect-to-database.py  - Test database connection"
          echo "  â€¢ test-s3-access.py       - Test S3 bucket access"
          echo "  â€¢ check-vector-status.sh  - Check Vector pipeline status"
          echo "  â€¢ manage-services.sh      - Manage all services"
          echo ""
          echo "Environment variables:"
          echo "  â€¢ AWS_DEFAULT_REGION: ${AWS::Region}"
          echo "  â€¢ S3_BUCKET: ${S3Bucket}"
          echo "  â€¢ DB_SECRET_ARN: ${AuroraSecret}"
          echo ""
          EOSCRIPT
          
          chmod +x /home/ec2-user/setup-data-pipeline-access.sh
          chown ec2-user:ec2-user /home/ec2-user/setup-data-pipeline-access.sh
          
          # Install VS Code extensions
          log "Installing VS Code extensions..."
          # Wait for VS Code server to be ready
          systemctl start code-server
          sleep 10
          
          # Install essential extensions for data pipeline development
          sudo -u ${VSCodeUser} --login code-server --install-extension AmazonWebServices.aws-toolkit-vscode --force || log "WARNING: Failed to install AWS Toolkit"
          sudo -u ${VSCodeUser} --login code-server --install-extension ms-python.python --force || log "WARNING: Failed to install Python extension"
          sudo -u ${VSCodeUser} --login code-server --install-extension ms-vscode.live-server --force || log "WARNING: Failed to install Live Server"
          sudo -u ${VSCodeUser} --login code-server --install-extension synedra.auto-run-command --force || log "WARNING: Failed to install Auto Run Command"
          sudo -u ${VSCodeUser} --login code-server --install-extension ms-toolsai.jupyter --force || log "WARNING: Failed to install Jupyter extension"
          sudo -u ${VSCodeUser} --login code-server --install-extension ms-vscode.vscode-json --force || log "WARNING: Failed to install JSON extension"
          sudo -u ${VSCodeUser} --login code-server --install-extension redhat.vscode-yaml --force || log "WARNING: Failed to install YAML extension"
          
          # Restart VS Code server to apply extensions
          systemctl restart code-server
          
          # Start other services
          log "Starting data pipeline services..."
          systemctl start vector
          
          # Enable services for auto-start
          systemctl enable vector
          
          # Try to start optional services
          systemctl start fluent-bit 2>/dev/null || log "WARNING: Fluent Bit service not available"
          systemctl start aws-kinesis-agent 2>/dev/null || log "WARNING: Kinesis Agent service not available"
          
          # Run data pipeline access setup
          log "Running data pipeline access setup..."
          sudo -u ec2-user /home/ec2-user/setup-data-pipeline-access.sh || log "WARNING: Data pipeline access setup had some issues"
          
          # Create comprehensive README
          log "Creating comprehensive README..."
          cat > /home/ec2-user/README.md << 'EOFREADME'
          # ðŸš€ Unified AWS Workshop Environment
          
          Welcome to your integrated AWS Workshop environment! This setup includes VS Code Server, data pipeline tools, and Aurora MySQL database.
          
          ## ðŸ“¦ What's Included
          
          ### Development Environment
          - **VS Code Server**: Web-based IDE accessible via CloudFront
          - **Python Environment**: Pre-configured virtual environment with data science packages
          - **Node.js**: For web development and AWS CDK
          - **Docker**: Container runtime for development
          
          ### Data Pipeline Tools
          - **Vector**: High-performance data collection and transformation
          - **Fluent Bit**: Lightweight log processor
          - **AWS Kinesis Agent**: Stream data to AWS Kinesis
          
          ### Database & Storage
          - **Aurora MySQL**: Managed MySQL cluster for game data
          - **S3 Bucket**: Data storage with VPC endpoint
          - **CloudWatch Logs**: Centralized logging
          
          ## ðŸ” Security Features
          - **Secrets Manager**: Secure password management
          - **CloudFront**: Global CDN with HTTPS
          - **VPC**: Isolated network environment
          - **IAM Roles**: Least-privilege access
          
          ## ðŸ› ï¸ Management Scripts
          
          ### Essential Scripts
          ```bash
          # Change VS Code password (updates Secrets Manager)
          ./change-vscode-password.sh
          
          # Manage all services (start/stop/restart/status)
          ./manage-services.sh status
          
          # Setup data pipeline tool access
          ./setup-data-pipeline-access.sh
          ```
          
          ### Data Pipeline Testing
          ```bash
          # Test database connection
          python3 connect-to-database.py
          
          # Test S3 access
          python3 test-s3-access.py
          
          # Check Vector status
          ./check-vector-status.sh
          ```
          
          ## ðŸš€ Getting Started
          
          1. **Access VS Code**: Use the CloudFront URL from stack outputs
          2. **Change Password**: Run `./change-vscode-password.sh` for security
          3. **Check Services**: Run `./manage-services.sh status`
          4. **Test Connections**: Run `./setup-data-pipeline-access.sh`
          
          ## ðŸ“Š Data Flow Architecture
          
          ```
          Application Logs â†’ Vector â†’ CloudWatch Logs / S3
                          â†“
          System Metrics â†’ Vector â†’ CloudWatch Logs
                          â†“
          Fluent Bit â†’ Vector â†’ File Output
                          â†“
          Kinesis Agent â†’ AWS Kinesis Stream
          ```
          
          ## ðŸ”§ Configuration Files
          
          - **Vector**: `/etc/vector/vector.yaml`
          - **Fluent Bit**: `/etc/fluent-bit/fluent-bit.conf`
          - **Kinesis Agent**: `/etc/aws-kinesis/agent.json`
          - **VS Code**: `~/.config/code-server/config.yaml`
          - **Nginx**: `/etc/nginx/conf.d/code-server.conf`
          
          ## ðŸŒ Access URLs
          
          - **VS Code (CloudFront)**: Check stack outputs for secure HTTPS URL
          - **VS Code (Direct)**: http://[PUBLIC-IP]:8080
          - **Health Check**: http://[PUBLIC-IP]/healthz
          
          ## ðŸ“ˆ Monitoring
          
          - **CloudWatch Logs**: `/aws/ec2/[STACK-NAME]/*`
          - **Service Status**: `systemctl status [service-name]`
          - **Vector Metrics**: Available via Vector API (if enabled)
          
          ## ðŸ” Troubleshooting
          
          ### Service Issues
          ```bash
          # Check all services
          ./manage-services.sh status
          
          # View service logs
          sudo journalctl -u code-server -f
          sudo journalctl -u vector -f
          sudo journalctl -u fluent-bit -f
          ```
          
          ### Connection Issues
          ```bash
          # Test database connection
          python3 connect-to-database.py
          
          # Test S3 access
          python3 test-s3-access.py
          
          # Check network connectivity
          curl -I http://localhost:8080/healthz
          ```
          
          ### VS Code Issues
          ```bash
          # Reset VS Code password
          ./change-vscode-password.sh
          
          # Restart VS Code service
          sudo systemctl restart code-server
          
          # Check VS Code logs
          sudo journalctl -u code-server -n 50
          ```
          
          ## ðŸ“ž Support
          
          If you encounter issues:
          1. Check service status with `./manage-services.sh status`
          2. Review logs with `sudo journalctl -u [service-name]`
          3. Test individual components with provided scripts
          4. Check CloudWatch Logs for detailed error information
          
          ## ðŸŽ¯ Workshop Objectives
          
          This environment is designed for:
          - **Data Pipeline Development**: Build and test data processing workflows
          - **Game Analytics**: Analyze game data with Aurora MySQL
          - **Cloud-Native Development**: Use AWS services in integrated workflows
          - **Real-time Processing**: Stream and process data with Vector and Fluent Bit
          
          ---
          
          **Environment Details:**
          - Region: ${AWS::Region}
          - Stack: ${AWS::StackName}
          - Instance Type: ${InstanceType}
          - Architecture: $ARCH_TYPE
          
          Happy coding! ðŸŽ‰
          EOFREADME
          
          chown ec2-user:ec2-user /home/ec2-user/README.md
          
          log "Workshop instance setup completed successfully!"
          log "VS Code Server: Configured with Secrets Manager integration"
          log "CloudFront: Configured for secure global access"
          log "Data Pipeline: Vector, Fluent Bit, and Kinesis Agent configured"
          log "Database: Aurora MySQL cluster ready"
          log "Management Scripts: Available in /home/ec2-user/"
          
          # Signal completion
          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WorkshopInstance --region ${AWS::Region}
    CreationPolicy:
      ResourceSignal:
        Timeout: PT15M

  # CloudFront Distribution for VS Code Server
  VSCodeCachePolicy:
    Type: AWS::CloudFront::CachePolicy
    Properties:
      CachePolicyConfig:
        Name: !Sub '${AWS::StackName}-VSCodeCachePolicy'
        DefaultTTL: 0
        MaxTTL: 0
        MinTTL: 0
        ParametersInCacheKeyAndForwardedToOrigin:
          EnableAcceptEncodingBrotli: false
          EnableAcceptEncodingGzip: false
          QueryStringsConfig:
            QueryStringBehavior: all
          HeadersConfig:
            HeaderBehavior: whitelist
            Headers:
              - Authorization
              - CloudFront-Forwarded-Proto
              - CloudFront-Is-Desktop-Viewer
              - CloudFront-Is-Mobile-Viewer
              - CloudFront-Is-Tablet-Viewer
              - Host
              - Referer
              - User-Agent
          CookiesConfig:
            CookieBehavior: all

  CloudFrontDistribution:
    Type: AWS::CloudFront::Distribution
    Properties:
      DistributionConfig:
        Enabled: true
        Comment: !Sub 'CloudFront distribution for ${AWS::StackName} VS Code Server'
        DefaultCacheBehavior:
          TargetOriginId: VSCodeServerOrigin
          ViewerProtocolPolicy: redirect-to-https
          CachePolicyId: !Ref VSCodeCachePolicy
          AllowedMethods:
            - DELETE
            - GET
            - HEAD
            - OPTIONS
            - PATCH
            - POST
            - PUT
          Compress: false
        Origins:
          - Id: VSCodeServerOrigin
            DomainName: !GetAtt WorkshopInstance.PublicDnsName
            CustomOriginConfig:
              HTTPPort: 80
              OriginProtocolPolicy: http-only
        PriceClass: PriceClass_100
        ViewerCertificate:
          CloudFrontDefaultCertificate: true

# Final Outputs
Outputs:
  # VS Code Access
  VSCodeURL:
    Description: VS Code Server URL via CloudFront
    Value: !Sub 
      - 'https://${Domain}'
      - Domain: !GetAtt CloudFrontDistribution.DomainName
  
  VSCodeDirectURL:
    Description: Direct VS Code Server URL (for troubleshooting)
    Value: !Sub 'http://${WorkshopInstance.PublicDnsName}:8080'
  
  VSCodePassword:
    Description: VS Code Server Password
    Value: !GetAtt SecretPlaintext.password
  
  # Instance Information
  InstanceId:
    Description: Workshop EC2 Instance ID
    Value: !Ref WorkshopInstance
  
  PublicIP:
    Description: Workshop Instance Public IP
    Value: !GetAtt WorkshopInstance.PublicIp
  
  PublicDNS:
    Description: Workshop Instance Public DNS
    Value: !GetAtt WorkshopInstance.PublicDnsName
  
  # Database Information
  DatabaseEndpoint:
    Description: Aurora MySQL Cluster Endpoint
    Value: !GetAtt AuroraDBCluster.Endpoint.Address
  
  DatabasePort:
    Description: Aurora MySQL Port
    Value: !GetAtt AuroraDBCluster.Endpoint.Port
  
  DatabaseName:
    Description: Database Name
    Value: game
  
  # Storage Information
  S3BucketName:
    Description: Workshop Data S3 Bucket
    Value: !Ref S3Bucket
  
  # Monitoring
  LogGroupName:
    Description: CloudWatch Log Group
    Value: !Ref WorkshopLogGroup
  
  # Network Information
  VPCId:
    Description: Workshop VPC ID
    Value: !Ref VPC
  
  SecurityGroupId:
    Description: Workshop Security Group ID
    Value: !Ref WorkshopSecurityGroup