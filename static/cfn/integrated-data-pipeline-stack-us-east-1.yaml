AWSTemplateFormatVersion: '2010-09-09'
Description: 'Integrated CloudFormation template for Data Pipeline with EC2, S3, Aurora MySQL cluster, VSCode, Vector, Fluent Bit, and Kinesis Agent'

Parameters:
  EC2InstanceType:
    Type: String
    Default: t3.medium
    AllowedValues: [t3.micro, t3.small, t3.medium, t3.large, m5.large, m5.xlarge]
    Description: EC2 instance type

  AllowedCIDR:
    Type: String
    Default: 0.0.0.0/0
    Description: CIDR block allowed to access services (default allows all)

  LatestAmiId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64
    Description: Latest Amazon Linux 2023 AMI ID

Resources:
  # Auto-generated Key Pair
  WorkshopKeyPair:
    Type: AWS::EC2::KeyPair
    Properties:
      KeyName: !Sub '${AWS::StackName}-workshop-keypair'
      KeyType: rsa
      KeyFormat: pem
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-workshop-keypair'

  # VPC and Networking
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: DataPipelineVPC

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: DataPipelineIGW

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 0, !GetAZs '' ]
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: DataPipeline-PublicSubnet1

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 1, !GetAZs '' ]
      CidrBlock: 10.0.2.0/24
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: DataPipeline-PublicSubnet2

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 0, !GetAZs '' ]
      CidrBlock: 10.0.3.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: DataPipeline-PrivateSubnet1

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [ 1, !GetAZs '' ]
      CidrBlock: 10.0.4.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: DataPipeline-PrivateSubnet2

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: DataPipeline-PublicRouteTable

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: AttachGateway
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet2
      RouteTableId: !Ref PublicRouteTable

  # Security Groups
  DataPipelineSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Data Pipeline instance
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: !Ref AllowedCIDR
          Description: SSH access
        - IpProtocol: tcp
          FromPort: 8080
          ToPort: 8080
          CidrIp: !Ref AllowedCIDR
          Description: VSCode Server access
        - IpProtocol: tcp
          FromPort: 8501
          ToPort: 8501
          CidrIp: !Ref AllowedCIDR
          Description: Streamlit access
        - IpProtocol: tcp
          FromPort: 24224
          ToPort: 24224
          CidrIp: 10.0.0.0/16
          Description: Fluent Bit forward input
        - IpProtocol: tcp
          FromPort: 8686
          ToPort: 8686
          CidrIp: 10.0.0.0/16
          Description: Vector API
        - IpProtocol: tcp
          FromPort: 2020
          ToPort: 2020
          CidrIp: 10.0.0.0/16
          Description: Fluent Bit HTTP
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          CidrIp: 10.0.0.0/16
          Description: MySQL access
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: All outbound traffic
      Tags:
        - Key: Name
          Value: DataPipeline-SecurityGroup

  # IAM Roles
  DataPipelineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        - arn:aws:iam::aws:policy/AdministratorAccess
      Policies:
        - PolicyName: KinesisAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kinesis:PutRecord
                  - kinesis:PutRecords
                  - kinesis:DescribeStream
                  - kinesis:ListStreams
                  - kinesis:CreateStream
                  - kinesis:DeleteStream
                  - firehose:PutRecord
                  - firehose:PutRecordBatch
                  - firehose:DescribeDeliveryStream
                  - firehose:ListDeliveryStreams
                Resource: '*'
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                Resource: '*'
        - PolicyName: LogsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:DescribeLogStreams
                  - logs:DescribeLogGroups
                Resource: '*'
        - PolicyName: SecretsManagerAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                  - secretsmanager:DescribeSecret
                Resource: !Ref AuroraSecret
        - PolicyName: AmazonLinux2023RepositoryAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: AmazonLinuxRepositoryS3Access
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                Resource:
                  - "arn:aws:s3:::al2023-repos-*"
                  - "arn:aws:s3:::al2023-repos-*/*"
                  - "arn:aws:s3:::amazonlinux-2-repos-*"
                  - "arn:aws:s3:::amazonlinux-2-repos-*/*"
              - Sid: VPCEndpointAccess
                Effect: Allow
                Action:
                  - ec2:DescribeVpcEndpoints
                  - ec2:DescribeRouteTables
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DescribeRegions
                Resource: '*'

  DataPipelineInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref DataPipelineRole

  # Lambda Execution Role for Secret Update
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SecretManagerAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                  - secretsmanager:PutSecretValue
                  - secretsmanager:ListSecrets
                Resource: 
                  - !Ref AuroraSecret
        - PolicyName: RDSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - rds:DescribeDBClusters
                Resource: '*'

  # Lambda function to update secrets
  UpdateSecretFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json

          def handler(event, context):
            if event['RequestType'] in ['Create', 'Update']:
              secret_manager = boto3.client('secretsmanager')
              rds = boto3.client('rds')
              
              secret_id = event['ResourceProperties']['SecretId']
              cluster_id = event['ResourceProperties']['ClusterId']
              
              try:
                response = rds.describe_db_clusters(DBClusterIdentifier=cluster_id)
                endpoint = response['DBClusters'][0]['Endpoint']
                
                secret = secret_manager.get_secret_value(SecretId=secret_id)
                secret_dict = json.loads(secret['SecretString'])
                secret_dict['host'] = endpoint
                
                secret_manager.put_secret_value(
                  SecretId=secret_id,
                  SecretString=json.dumps(secret_dict)
                )
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                print(f"Error updating secret: {str(e)}")
                cfnresponse.send(event, context, cfnresponse.FAILED, {})
                return
            else:
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

  # S3 Bucket
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'datapipelinebucket${AWS::AccountId}${AWS::Region}'
      Tags:
        - Key: Name
          Value: DataPipeline-S3Bucket

  # Aurora MySQL Cluster
  AuroraDBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet group for Aurora DB
      SubnetIds: 
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2
      Tags:
        - Key: Name
          Value: DataPipeline-DBSubnetGroup

  # Secrets Manager Secret
  AuroraSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub 'gamedb1cluster-${AWS::StackName}'
      Description: Secrets for Aurora MySQL Cluster
      SecretString: !Sub '{"username":"dbadmin","password":"12345678","engine":"mysql","host":"gamedb1cluster.cluster-${AWS::Region}.rds.amazonaws.com","port":3306,"dbClusterIdentifier":"gamedb1cluster","dbname":"game"}'

  AuroraDBCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      DatabaseName: game
      DBClusterIdentifier: !Sub 'gamedb1cluster-${AWS::StackName}'
      MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref AuroraSecret, ':SecretString:username}}' ]]
      MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref AuroraSecret, ':SecretString:password}}' ]]
      DBSubnetGroupName: !Ref AuroraDBSubnetGroup
      VpcSecurityGroupIds: 
        - !Ref DataPipelineSecurityGroup
      Tags:
        - Key: Name
          Value: DataPipeline-AuroraCluster

  AuroraDBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      Engine: aurora-mysql
      DBClusterIdentifier: !Ref AuroraDBCluster
      DBInstanceClass: db.r5.large
      PubliclyAccessible: false
      Tags:
        - Key: Name
          Value: DataPipeline-AuroraInstance
  
  UpdateSecretCustomResource:
    Type: Custom::UpdateSecret
    DependsOn:
      - AuroraDBCluster
    Properties:
      ServiceToken: !GetAtt UpdateSecretFunction.Arn
      SecretId: !Ref AuroraSecret
      ClusterId: !Ref AuroraDBCluster

  # S3 VPC Endpoint
  S3Endpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal: '*'
            Action:
              - 's3:*'
            Resource:
              - !Sub 'arn:aws:s3:::${S3Bucket}'
              - !Sub 'arn:aws:s3:::${S3Bucket}/*'
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcId: !Ref VPC
      RouteTableIds:
        - !Ref PublicRouteTable

  # CloudWatch Log Groups
  DataPipelineLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/ec2/data-pipeline
      RetentionInDays: 7

  FluentBitLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/ec2/fluent-bit
      RetentionInDays: 7

  # EC2 Instance
  DataPipelineInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref LatestAmiId
      InstanceType: !Ref EC2InstanceType
      KeyName: !Ref WorkshopKeyPair
      SubnetId: !Ref PublicSubnet1
      SecurityGroupIds:
        - !Ref DataPipelineSecurityGroup
      IamInstanceProfile: !Ref DataPipelineInstanceProfile
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 30
            VolumeType: gp3
            DeleteOnTermination: true
      Tags:
        - Key: Name
          Value: DataPipelineInstance
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          
          # 모든 출력을 로그 파일에도 저장
          exec > >(tee /var/log/user-data.log) 2>&1
          
          # Amazon Linux 버전 감지 및 패키지 매니저 설정
          if grep -q "Amazon Linux release 2023" /etc/system-release 2>/dev/null || [ -f /etc/amazon-linux-release ]; then
            echo "Amazon Linux 2023 감지됨"
            PACKAGE_MANAGER="dnf"
            
            # Amazon Linux 2023 저장소 문제 해결
            echo "Amazon Linux 2023 저장소 설정 중..."
            
            # 문제가 있는 저장소 비활성화
            dnf config-manager --disable kernel-livepatch 2>/dev/null || true
            
            # 캐시 정리
            dnf clean all
            rm -rf /var/cache/dnf/*
            
            # 기본 저장소만 사용하여 메타데이터 업데이트
            dnf makecache --refresh --disablerepo=kernel-livepatch 2>/dev/null || true
            
          else
            echo "Amazon Linux 2 감지됨"
            PACKAGE_MANAGER="yum"
            yum clean all
          fi
          
          # 기본 패키지 설치
          echo "✅ 1단계: 기본 패키지 설치 시작"
          
          if [ "$PACKAGE_MANAGER" = "dnf" ]; then
            # Amazon Linux 2023용 패키지 설치
            dnf update -y --skip-broken --disablerepo=kernel-livepatch
            dnf install -y curl unzip tar wget --skip-broken --disablerepo=kernel-livepatch
            
            # Java 설치 (Amazon Linux 2023에서는 다른 패키지명 사용)
            dnf install -y java-11-openjdk --skip-broken --disablerepo=kernel-livepatch || \
            dnf install -y java-17-openjdk --skip-broken --disablerepo=kernel-livepatch || \
            echo "Java 설치 실패, 계속 진행..."
          else
            # Amazon Linux 2용 패키지 설치
            yum update -y
            yum install -y curl unzip tar wget java-11-openjdk
          fi
          
          echo "✅ 1단계 완료: 기본 패키지 설치"
          
          # Node.js 설치
          echo "✅ 2단계: Node.js 설치 시작"
          
          if [ "$PACKAGE_MANAGER" = "dnf" ]; then
            # Amazon Linux 2023용 Node.js 설치
            curl -fsSL https://rpm.nodesource.com/setup_18.x | bash -
            dnf install -y nodejs --skip-broken --disablerepo=kernel-livepatch || \
            dnf install -y npm --skip-broken --disablerepo=kernel-livepatch || \
            echo "Node.js 설치 실패, 계속 진행..."
          else
            # Amazon Linux 2용 Node.js 설치
            curl -fsSL https://rpm.nodesource.com/setup_18.x | bash -
            yum install -y nodejs
          fi
          
          echo "✅ 2단계 완료: Node.js 설치"
          
          # VSCode Server 설치
          echo "✅ 3단계: VSCode Server 설치 시작"
          
          # ec2-user로 VSCode Server 설치
          sudo -u ec2-user bash -c 'curl -fsSL https://code-server.dev/install.sh | sh'
          
          # 설치 확인
          if command -v code-server >/dev/null 2>&1; then
              echo "✅ VSCode Server 설치 성공"
          else
              echo "❌ VSCode Server 설치 실패, 재시도 중..."
              # 재시도
              curl -fsSL https://code-server.dev/install.sh | bash -s --
              if command -v code-server >/dev/null 2>&1; then
                  echo "✅ VSCode Server 재설치 성공"
              else
                  echo "❌ VSCode Server 설치 완전 실패"
                  exit 1
              fi
          fi
          
          echo "✅ 3단계 완료: VSCode Server 설치"
          
          # VSCode 설정 디렉토리 생성 및 설정 파일 생성
          echo "✅ 4단계: VSCode 설정 및 서비스 설정 시작"
          
          # 설정 디렉토리 생성
          mkdir -p /home/ec2-user/.config/code-server
          mkdir -p /home/ec2-user/.local/share/code-server/User/
          mkdir -p /home/ec2-user/workspace
          
          # VSCode 서버 설정 파일 생성
          cat > /home/ec2-user/.config/code-server/config.yaml << 'EOF'
          cert: false
          auth: password
          password: temp123!
          bind-addr: 0.0.0.0:8080
          EOF
          
          # VSCode 사용자 설정 파일 생성
          cat > /home/ec2-user/.local/share/code-server/User/settings.json << 'EOF'
          {
            "extensions.autoUpdate": false,
            "workbench.colorTheme": "Default Dark+",
            "terminal.integrated.shell.linux": "/bin/bash",
            "python.defaultInterpreterPath": "/home/ec2-user/workspace/venv/bin/python",
            "files.autoSave": "afterDelay",
            "editor.fontSize": 14,
            "terminal.integrated.fontSize": 13
          }
          EOF
          
          # 권한 설정
          chown -R ec2-user:ec2-user /home/ec2-user/.config
          chown -R ec2-user:ec2-user /home/ec2-user/.local
          chown -R ec2-user:ec2-user /home/ec2-user/workspace
          
          # Python 가상환경 생성 (ec2-user로 실행)
          if command -v python3 >/dev/null 2>&1; then
            sudo -u ec2-user python3 -m venv /home/ec2-user/workspace/venv
            sudo -u ec2-user /home/ec2-user/workspace/venv/bin/pip install --upgrade pip
            sudo -u ec2-user /home/ec2-user/workspace/venv/bin/pip install pandas numpy matplotlib seaborn streamlit mysql-connector-python boto3
          else
            echo "Python3이 설치되지 않음, 가상환경 생성 건너뜀"
          fi
          
          # systemd 서비스로 등록 및 시작
          echo "systemd 서비스 시작 중..."
          systemctl enable --now code-server@ec2-user
          
          # 서비스 시작 대기
          sleep 5
          
          # 필수 VSCode 확장 프로그램 설치
          echo "VSCode 확장 프로그램 설치 중..."
          sudo -u ec2-user code-server --install-extension AmazonWebServices.aws-toolkit-vscode --force || echo "확장 프로그램 설치 실패, 계속 진행..."
          
          # 서비스 재시작
          systemctl restart code-server@ec2-user
          
          echo "✅ 4단계 완료: VSCode 설정 및 서비스 설정"
          
          # 관리 스크립트 생성
          echo "✅ 5단계: 관리 스크립트 생성 시작"
          cat > /home/ec2-user/change-vscode-password.sh << 'EOF'
          #!/bin/bash
          echo "=================================================="
          echo "VSCode 서버 비밀번호 변경 스크립트"
          echo "=================================================="
          echo "현재 임시 비밀번호: temp123!"
          echo ""
          echo "보안을 위해 반드시 비밀번호를 변경해주세요."
          echo ""
          
          # 비밀번호 입력
          read -s -p "새 비밀번호를 입력하세요: " NEW_PASSWORD
          echo ""
          read -s -p "비밀번호를 다시 입력하세요: " CONFIRM_PASSWORD
          echo ""
          
          # 비밀번호 확인
          if [ "$NEW_PASSWORD" != "$CONFIRM_PASSWORD" ]; then
              echo "❌ 비밀번호가 일치하지 않습니다."
              exit 1
          fi
          
          # 비밀번호 길이 확인
          PASSWORD_LENGTH=$(echo -n "$NEW_PASSWORD" | wc -c)
          if [ $PASSWORD_LENGTH -lt 8 ]; then
              echo "❌ 비밀번호는 최소 8자 이상이어야 합니다."
              exit 1
          fi 
          
          echo "비밀번호를 변경하는 중..."
          
          # VSCode 설정 파일 업데이트
          sed -i "s/password: .*/password: $NEW_PASSWORD/" /home/ec2-user/.config/code-server/config.yaml
          
          # 서비스 재시작
          sudo systemctl restart code-server@ec2-user
          
          # 서비스 상태 확인
          sleep 3
          if systemctl is-active --quiet code-server@ec2-user; then
              echo "✅ 비밀번호가 성공적으로 변경되었습니다."
              echo "✅ VSCode 서버가 재시작되었습니다."
              echo ""
              echo "이제 새 비밀번호로 VSCode에 접속할 수 있습니다."
              echo "URL: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8080"
          else
              echo "❌ VSCode 서버 재시작에 실패했습니다."
              echo "서비스 상태를 확인해주세요: sudo systemctl status code-server@ec2-user"
              exit 1
          fi
          EOF
          
          chmod +x /home/ec2-user/change-vscode-password.sh
          chown ec2-user:ec2-user /home/ec2-user/change-vscode-password.sh
          
          # README 파일 생성
          cat > /home/ec2-user/README.md << 'EOF'
          # 🚀 VSCode 서버 환경
          
          ## 📦 설치된 도구들
          - **VSCode Server**: 웹 기반 통합 개발 환경 (포트: 8080)
          - **Node.js**: JavaScript 런타임
          
          ## 🚀 시작하기
          
          1. **비밀번호 변경**: `./change-vscode-password.sh`
          2. **VSCode 접속**: http://[PUBLIC-IP]:8080
          
          ## 🔧 서비스 관리
          
          ```bash
          # 서비스 상태 확인
          sudo systemctl status code-server@ec2-user
          
          # 서비스 재시작
          sudo systemctl restart code-server@ec2-user
          
          # 로그 확인
          sudo journalctl -u code-server@ec2-user -f
          ```
          
          ## 🛠️ 수동 패키지 설치 (필요시)
          
          ```bash
          # Amazon Linux 2023에서 Java 설치
          sudo dnf install -y java-11-openjdk --disablerepo=kernel-livepatch
          
          # Kinesis Agent 수동 설치
          sudo dnf install -y aws-kinesis-agent --disablerepo=kernel-livepatch
          ```
          
          ## 📞 지원
          문제가 발생하면 `sudo journalctl -u code-server@ec2-user -n 50`으로 로그를 확인하세요.
          EOF
          
          chown ec2-user:ec2-user /home/ec2-user/README.md
          echo "✅ 5단계 완료: 관리 스크립트 생성"
          
          echo "🎉 VSCode 서버 설치 완료!"
          echo "접속 URL: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8080"
          echo "임시 비밀번호: temp123!"
          echo "반드시 ./change-vscode-password.sh 스크립트로 비밀번호를 변경하세요!"
          
          # 설치 상태 요약
          echo ""
          echo "=== 설치 상태 요약 ==="
          echo "VSCode Server: $(command -v code-server >/dev/null 2>&1 && echo '✅ 설치됨' || echo '❌ 설치 실패')"
          echo "Node.js: $(command -v node >/dev/null 2>&1 && echo '✅ 설치됨' || echo '❌ 설치 실패')"
          echo "Java: $(command -v java >/dev/null 2>&1 && echo '✅ 설치됨' || echo '❌ 설치 실패')"
          echo "Python3: $(command -v python3 >/dev/null 2>&1 && echo '✅ 설치됨' || echo '❌ 설치 실패')"
Outputs:
  InstanceId:
    Description: Instance ID of the Data Pipeline server
    Value: !Ref DataPipelineInstance
    Export:
      Name: !Sub "${AWS::StackName}-InstanceId"

  PublicIP:
    Description: Public IP address of the Data Pipeline server
    Value: !GetAtt DataPipelineInstance.PublicIp
    Export:
      Name: !Sub "${AWS::StackName}-PublicIP"

  VSCodeURL:
    Description: VSCode Server URL
    Value: !Sub "http://${DataPipelineInstance.PublicIp}:8080"
    Export:
      Name: !Sub "${AWS::StackName}-VSCodeURL"

  SSHCommand:
    Description: SSH command to connect to the instance
    Value: !Sub "ssh -i ${AWS::StackName}-workshop-keypair.pem ec2-user@${DataPipelineInstance.PublicIp}"
    Export:
      Name: !Sub "${AWS::StackName}-SSHCommand"

  KeyPairName:
    Description: Name of the auto-generated key pair
    Value: !Ref WorkshopKeyPair
    Export:
      Name: !Sub "${AWS::StackName}-KeyPairName"

  VSCodeInitialPassword:
    Description: Initial VSCode password (change immediately)
    Value: "temp123!"
    Export:
      Name: !Sub "${AWS::StackName}-VSCodeInitialPassword"

  S3BucketName:
    Description: S3 Bucket Name
    Value: !Ref S3Bucket
    Export:
      Name: !Sub "${AWS::StackName}-S3BucketName"

  AuroraClusterEndpoint:
    Description: Aurora Cluster Endpoint
    Value: !GetAtt AuroraDBCluster.Endpoint.Address
    Export:
      Name: !Sub "${AWS::StackName}-AuroraEndpoint"

  AuroraSecretARN:
    Description: ARN of Aurora Secret
    Value: !Ref AuroraSecret
    Export:
      Name: !Sub "${AWS::StackName}-AuroraSecretARN"

  VpcId:
    Description: VPC ID
    Value: !Ref VPC
    Export:
      Name: !Sub "${AWS::StackName}-VpcId"

  PublicSubnet1:
    Description: Public Subnet 1 ID
    Value: !Ref PublicSubnet1
    Export:
      Name: !Sub "${AWS::StackName}-PublicSubnet1"

  PublicSubnet2:
    Description: Public Subnet 2 ID
    Value: !Ref PublicSubnet2
    Export:
      Name: !Sub "${AWS::StackName}-PublicSubnet2"

  PrivateSubnet1:
    Description: Private Subnet 1 ID
    Value: !Ref PrivateSubnet1
    Export:
      Name: !Sub "${AWS::StackName}-PrivateSubnet1"

  PrivateSubnet2:
    Description: Private Subnet 2 ID
    Value: !Ref PrivateSubnet2
    Export:
      Name: !Sub "${AWS::StackName}-PrivateSubnet2"

  S3VPCEndpointId:
    Description: S3 VPC Endpoint ID
    Value: !Ref S3Endpoint
    Export:
      Name: !Sub "${AWS::StackName}-S3VPCEndpointId"
